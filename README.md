# Aligno: IT Job Search Engine
![Python 3.13](https://img.shields.io/badge/python-3.13-blue) ![asyncpg](https://img.shields.io/badge/asyncpg-0.29.0-blue) ![Playwright](https://img.shields.io/badge/playwright-1.52-blue) ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15.3-blue)
## ğŸš€ Overview

Aligno is a web application for collecting, processing and analyzing job offers from JustJoin.it. The main goals are:
1. Automatic retrieval and updating of the job offers database.
2. Presentation of market statistics via a dashboard.
3. Interactive job search based on user preferences and skills.
4. Generation of a personalized CV for a specific job posting.

## ğŸ”§ Key Features

1. **JustJoin.it Scraper**
   - Playwright-based scraper collecting job-offer links and details from JustJoin.it.
   - Updates PostgreSQL database by inserting new offers and purging stale ones.


2. **Market overview** (To do)
   - Presents market statistics via a dashboard.
   - Displays insights such as:
     - Number of job offers per month, technology, location etc.
     - Most popular technologies and skills.
     - Dependencies between salary and technology.

3. **Job search** (To do)
   - Allows users to search for job offers based on their skills and preferences.
   - Provides a personalized job search experience.
   - Displays job offers sorted by match to the user's skills and preferences.

4. **CV generation** (To do)
   - Generates a personalized CV for a specific job posting.
   - Allows users to customize their CV based on the job offer.
   - Provides an option to download the CV in various formats (PDF, DOCX, etc.).

## ğŸ“ Repository Structure

```
Aligno/
â”œâ”€ src/                                # Source code catalog
â”‚  â”œâ”€ sql/                             # SQL initialization scripts
â”‚  â”‚  â”œâ”€ 01_offers.sql                 # Table definition
â”‚  â”‚  â””â”€ 02_offers_processed_view.sql  # View definition
â”‚  â”œâ”€ validation/                      # Data validation module
â”‚  â”‚  â”œâ”€ __init__.py                   # Package initialization
â”‚  â”‚  â”œâ”€ models.py                     # Pydantic models for data validation
â”‚  â”‚  â”œâ”€ config.py                     # Configuration validation
â”‚  â”‚  â””â”€ validators.py                 # Custom validators and helpers
â”‚  â”œâ”€ scraper/                         # Package for scraper functionality
â”‚  â”‚  â”œâ”€ __main__.py                   # Package API
â”‚  â”‚  â”œâ”€ cli.py                        # CLI module with argument parsing and orchestration
â”‚  â”‚  â”œâ”€ db.py                         # Database connection and schema management
â”‚  â”‚  â””â”€ scrape_core.py                # Playwright browser init and scraping logic
â”œâ”€ .env.example                        # Environment variables example
â”œâ”€ requirements.txt                    # Python dependencies
â”œâ”€ mypy.ini                            # Mypy configuration
â””â”€ README.md                           # Project documentation
```

## ğŸ› ï¸ Installation & Setup

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd Aligno
   ```

2. **Set up virtual environment:**
   ```bash
   # Virtual environment is already included in the project
   source venv/bin/activate  # On macOS/Linux
   # or
   venv\Scripts\activate     # On Windows
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables:**
   ```bash
   cp .env.example .env
   # Edit .env with your actual values
   ```

5. **Run the scraper:**
   ```bash
   cd src
   ../venv/bin/python -m scraper
   ```

6. **Run API (when implemented):**
   ```bash
   cd src
   ../venv/bin/python -m api
   ```

## âš™ï¸ Configuration

Create a `.env` file in the root directory with the following variables (copy from `.env.example`):

```bash
# Database Configuration
DATABASE_URL=postgresql://user:password@localhost:5432/aligno_db
# Alternative: individual database settings
DB_USER=aligno
DB_PASSWORD=your_password_here
DB_HOST=localhost
DB_PORT=5432
DB_NAME=aligno_db


# Scraper Configuration
HEADLESS=true  # Set to false for debugging (shows browser window)
BATCH_SIZE=500  # Batch size for database operations
SCROLL_PAUSE=0.512  # Pause between scrolls in seconds
MAX_IDLE=5  # Maximum idle scrolls before stopping
SCRAPER_TIMEOUT=30000  # Timeout for page operations in milliseconds
MAX_OFFERS=100  # Limit number of offers for debugging (None = no limit)
```

### ğŸ”’ Data Validation & Security

The application now includes comprehensive data validation using Pydantic models:

- **Input Validation**: All scraped data is validated before database insertion
- **Environment Validation**: Configuration is validated on startup
- **SQL Injection Protection**: Database names and queries are sanitized
- **Data Sanitization**: All string inputs are cleaned and validated
- **Error Handling**: Robust error handling with proper logging

### ğŸ› Debugging & Development

For development and debugging purposes, you can limit the number of offers scraped:

```bash
# Limit to 50 offers for quick testing
MAX_OFFERS=50

# Or disable limit for full scraping
MAX_OFFERS=
```

**Debug Tips:**
- Set `HEADLESS=false` to see the browser window during scraping
- Use `MAX_OFFERS=10` for very quick testing
- Monitor logs for validation errors and data quality issues

### ğŸš¨ Required Environment Variables

The following environment variables are **required**:
- Either `DATABASE_URL` OR `DB_PASSWORD` (if using individual DB settings)

## ğŸ“‘ Code Highlights

- **validation/** - data validation and configuration management:
   - `models.py`: Pydantic models for validating job offers and configuration
   - `config.py`: Environment variable validation and configuration loading
   - `validators.py`: Custom validators and data sanitization utilities

- **scraper/** - package that provides:
   - `__main__.py`: package API for the scraper.
   - `cli.py`: CLI wrapper with environment validation and error handling.
   - `db.py`: handles asyncpg connection, database creation, inserts and purges with validation.
   - `scrape_core.py`: contains browser initialization, scrolling, link collection, and validated offer parsing.


## ğŸ“ Future Improvements

**Market overview:**
   * To choose frontend stack (React, Vue, Angular)
   * To choose chart library (Chart.js, Recharts, D3)
   * Components:
     * MVP
       * Total number of job offers
       * Top technologies
       * Salary statistics
       * Global filtering (by locations, operating modes, experience, categories etc.)
     * Future
       * Alerts
       * Trends
     * Nice to have
       * Top companies
       * Heatmaps

**Job search:**
   * API (Flask/Django, FastAPI, Node/Express)
   * To choose frontend stack (React, Vue, Angular)

**CV generation:**
   * Template (HTML/CSS, Markdown or other)
   * Optional: template engine (Jinja2, Handlebars, etc.)
   * Optional: AI generated sections (About me etc.)

**Scraper:**
   * To consider: Scheduling (cron/GitHub Actions)
   * To consider: Support for other job portals
   * To consider: Store configuration constants (URLs, timeouts, selectors) in `constants.py` or `config.toml`